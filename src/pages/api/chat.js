import { Configuration, OpenAIApi } from 'openai';

const openai = new OpenAIApi(
  new Configuration({
    apiKey: process.env.OPENAI_API_KEY,
  })
)

export default async function handler(req, res) {
  console.log('Requesting ChatGPT prompt or question', req.body)
  // const { language, topic, notes } = req.body
  const { conversationData } = req.body

  try {
    const completionRequest = {
      model: 'gpt-3.5-turbo-16k',
      // model: 'gpt-4',
      messages: [
        {
          role: 'system',
          content:
            'You are a helpful assistant.',
        },
        {
          role: 'assistant',
          content:
            `This section provides a catalog of prompt patterns that can be used to improve the performance of LLM models and assist in various software-related tasks. Here is a summary of each pattern:

            B. Meta Lang Creation Pattern: This pattern allows users to use alternate languages or notations with LLM and teach it the semantics of those languages. It is useful for clarifying problems or ideas that may be clearer in non-English languages.
            
            C. Output Automater: This pattern aims to automate manual steps by instructing LLM to generate executable code or text that automates certain tasks. It is particularly useful in software engineering, terminal commands, cloud operations, and file reorganization.
            
            D. Interaction Pattern: This pattern focuses on the interaction between LLM and the user, where the user provides information and LLM responds with guidance or performs a specific task. It allows for a conversational approach to achieve a specific goal.
            
            E. Persona Pattern: This pattern involves LLM assuming the role of a specific persona or character, such as a security expert or a Linux terminal. The outputs generated by LLM are tailored to match the characteristics and expertise of the chosen persona.
            
            F. Refinement Pattern: The refinement pattern aims to improve user questions or prompts to obtain better answers from LLM. It helps non-expert users in formulating questions and incorporates additional information to provide more accurate answers.
            
            G. Alt. Approaches Pattern: This pattern encourages LLM to suggest alternative approaches or strategies for a given task. It helps overcome cognitive biases and familiarizes users with different methods.
            
            H. Cognitive Verifier: This pattern involves LLM asking sub-questions to reason and provide a combined answer. It helps in obtaining more complete answers and encourages critical thinking.
            
            I. Fact Check List: This pattern involves LLM listing the facts used to generate the output and prompting the user to verify their accuracy. It helps in mitigating the risk of generating convincing but false information.
            
            J. Template Pattern: This pattern involves providing LLM with a template or format for the desired output. LLM fills in the placeholders in the template with the generated text or information. It is useful when the format of the output is unknown to LLM.
            
            K. Infinite Gen. Pattern: This pattern allows LLM to auto-generate an unending series of output, limiting user typing and reducing manual work. It can be used to create variations of output based on the same prompt.
            
            L. Visualization Generator Pattern: This pattern combines text generation with visualization tools to create visual representations of the output generated by LLM. It helps users grasp complex concepts more easily.
            
            M. Game Play Pattern: This pattern involves creating a game around a specific topic and using LLM as a game guide. Users provide the rules, and LLM generates the game content. It helps in generating scenarios and reducing manual work.
            
            N. Reflection Pattern: This pattern involves LLM explaining the rationale behind its answers to clarify confusion, uncover assumptions, and address shortcomings. It helps in understanding the model's processing and improving prompts.
            
            O. Refusal Breaker Pattern: This pattern assists users in rephrasing or altering their questions when LLM refuses to answer. It can help overcome LLM's lack of knowledge or understanding but should be used ethically.
            
            P. Context Manager Pattern: This pattern allows users to define and manage the context for LLM. Users can specify what should be considered, ignored, or reset, giving more control over LLM's understanding and output.
            
            Q. Recipe Pattern: This pattern involves generating step-by-step procedures or recipes using partial information or alternative possibilities. It helps users who may not know the exact steps or want to explore different options.
            
            These prompt patterns can be used to structure prompt discussions, provide examples, and classify patterns for more efficient communication with LLM models.`,
        },
        {
          role: 'user',
          content: `From now on, I would like you to ask me questions to write the best prompt possible for ChatGPT and my goals. When you have enough information to formulate a prompt, write a prompt for the user to give to ChatGPT in the form of "Prompt: <generated_prompt>". Ensure the prompt is in "I" form, as I will be entering what you give me verbatim. Ask one question at a time in the form of "Question <int>:" and explain your reasoning along the way.`,
        },
      ],
    }

    for (const QuestionResponse of conversationData) {
      completionRequest.messages.push({
        role: 'assistant',
        content: `${QuestionResponse.content}`
      })
      completionRequest.messages.push({
        role: 'user',
        content: `${QuestionResponse.userResponse}`
      })
    }

    const questionPrompt = await (async () => {
      console.log('Sending completion request', completionRequest)
      const completion = await openai.createChatCompletion(completionRequest)
      console.log('Received completion', completion.data)
      return completion.data.choices[0].message?.content ?? 'Failed response'
    })()

    if (questionPrompt) {
      res.status(200).json({
        questionPrompts: [{ questionPrompt }],
      })
    } else {
      res.status(400).json({ questionPrompts: [], errors: ['No message from OpenAI'] })
    }

  } catch (e) {
    console.error('Error in backend', e.response)
    res.status(500).json({
      questionPrompts: [],
      errors: [`Error calling openai: ${e.message}`],
    })
  }
}
